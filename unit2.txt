Unit 2: Peer-to-Peer Networking
2.1

Introduction

2.2

P2P Topologies

2.3

P2P Applications

2.4

P2P and the Internet

2.5

Unstructured overlay topology

2.6

Structured overlay topology

Reading: Chapters 29, Textbook – Forouzan

Unit 2 - 2

2.1 Introduction

Unit 2 - 3

Definition of P2P Systems
There is no universally accepted definition of P2P systems, although there
are many definitions, and there are some common characteristics shared by
most P2P systems:
A ‘peer’ is a computer that can act as both server and/or client.

A P2P system should consist of at least two or more peers.
Peers should be able to exchange resources directly among themselves.
Such resources include files, storages, information, central processing
unit (CPU) power and knowledge.
Dedicated servers may or may not be present in a P2P system
depending on the nature of the applications.
P2P systems without dedicated servers are sometimes described as
‘pure’ P2P systems.
Peers can join and/or leave the system freely.

Unit 2 - 4

Benefits of P2P Systems
Workload is spread to all peers
It is possible to have millions of computers in a P2P network, which can deliver
huge resources and power.
Maximize system utilization
Many office computers are not used at all from 5 pm to 9 am in the next
morning. P2P computer can use these resources and thus maximize the
utilization.
No central point of failure
E.g., the Internet and the Web do not have a central point of failure.

P2P network will still function when some of its peers are not working properly.
Thus it is more fault tolerant than other systems (e.g. C/S).
Scalability
Since every peer is alike, it is possible to add more peers to the system and scale
to larger networks.

Unit 2 - 5

Disadvantages of P2P Computing
The peer will be more susceptible to hackers’ attacks.
It is difficult to enforce standards in P2P systems.
A P2P network cannot guarantee that a particular resource will be available
all the time.

For example, the owner may shut down his/her computer or delete a
file.
It is difficult to predict the overall performance of a system.
It is difficult to prevent illegal uploading and downloading of copyrighted
materials.
A popular P2P system can generate enormous amount of network traffic.
As a result, some universities did not allow their students to access
some P2P applications inside the campus.

Unit 2 - 6

2.2 P2P Topologies

Unit 2 - 7

P2P Topologies
Centralized
Ring

Hierarchical
Decentralized

Hybrid

Unit 2 - 8

Centralized Topology
Centralized systems are the most familiar form of topology.
Typically seen as the client/server pattern used by
databases, web servers,
and other simple distributed systems.
All function and information is centralized
into one server with many clients connecting
directly to the server to send and receive
information.
Many "peer-to-peer" applications
also have a centralized component.
The original Napster's search architecture
was centralized, although the file sharing was not.

Unit 2 - 9

Ring Topology
A single centralized server cannot handle high clients load
A common solution is to use a cluster of machines
arranged in a ring to act as a distributed servers.
Communication between the nodes
coordinates state-sharing, to provide
identical function
With fail-over and
load-balancing capabilities.
Ring systems are generally built on
the assumption that
the machines are all nearby on the network

and owned by a single organization.

Unit 2 - 10

Hierarchical Topology
Hierarchical systems have a long history on the Internet
The best-known hierarchical system on the Internet
is the Domain Name Service,
where authority flows from
the root name-servers to the server
for the registered name.

The Network Time Protocol (NTP) is
a protocol for synchronizing the clocks
of computer systems over networks
there are root time servers that have authoritative clocks; other
computers synchronize to root time servers in a self-organizing tree.

Unit 2 - 11

Decentralized Topology
This is the opposite of centralized topology
All peers communicate symmetrically and have equal roles.
Decentralized systems are not new;
the Internet routing architecture is
largely decentralized, with the
Border Gateway Protocol between
various autonomous systems.
Gnutella is probably the most "pure"
decentralized system used in practice today

Unit 2 - 12

Hybrid Topology: Centralized + Ring
Real-world systems often combine several topologies into one system,
making a hybrid topology.
Nodes typically play multiple roles
in such a system.

For example, a node might have
a centralized interaction with one
part of the system, while being
part of a ring with other nodes.
Centralized + Ring
Most web server applications often have
a ring of servers for load balancing and failover.

The system as a whole is a hybrid: a centralized system for clients
where the server is itself a ring.

Unit 2 - 13

Hybrid Topology: Centralized + Decentralized
Centralized + Decentralized
Advancing an architecture of
centralized systems embedded
in decentralized systems.

Most peers have a centralized
relationship to a "supernode"
forwarding all file queries to
this server
But instead of supernodes being
standalone servers, they band
themselves together in a
decentralized network, propagating queries.

Unit 2 - 14

Evaluating Topologies
Manageability
How hard is it to keep working, in terms of updating, repairing, and logging?
Information coherence
If a bit of data is found in the system, is that data correct? (Non-repudiation and
data consistency)
Extensibility
How easy is the system to grow?
Fault tolerance
How well can it handle failures?
Resistance to legal or political intervention
How hard is it to shut down? (Can be good or bad)
Security

How hard is the system to be attacked?
Scalability
How big can the system grow?

Unit 2 - 15

Centralized
Manageable
Coherent
Extensible

✓ System is all in one place
✓ All information is in one place
X Resources (CPU/Data) can only be
added to the central system

Fault Tolerant
Secure

X Single point of failure
✓ Simply only one host that needs
to be protected

Lawsuit-proof
Scalable

X Easy to shut down
?

Limited by the capacity of the server.

Unit 2 - 16

Ring
Manageable
Coherent
Extensible

Fault Tolerant
Secure
Lawsuit-proof
Scalable

✓ Typically have a single owner, with
rules for relationships
✓ Easy logic for state

X Only ring owner can add (a user still
needs the owner's permission to add
a resource like a music file or a Web
page into the ring)
✓ Fail-over to next host
✓ As long as ring has one owner

X Shut down owner
✓ Just add more hosts
Unit 2 - 17

Hierarchical
Manageable

½ Chain of authority

Coherent

½ Cache consistency

Extensible

½ Add more leaves, rebalance

Fault Tolerant

½ the root is still a single point
of failure

Secure
Lawsuit-proof
Scalable

½ It is not just the root that is a risk
X Just shut down the root
✓ Hugely scalable – DNS

Unit 2 - 18

Decentralized
Manageable
Coherent

Extensible
Fault Tolerant
Secure
Lawsuit-proof
Scalable

X Very difficult, many owners
X Difficult, unreliable peers

✓ Anyone can join in!
✓ Redundancy
X Difficult, open research
✓ No one to sue
?

Theory – yes : Practice – no

Unit 2 - 19

Centralized + Ring
Manageable

✓ Just manage the ring

Coherent

✓ As coherent as ring

Extensible

X No more than ring

Fault Tolerant

✓ Ring is a huge win

Secure
Lawsuit-proof
Scalable

✓ As secure as ring
X Still single place to shut down
✓ Ring is a huge win

Common architecture for web applications
Unit 2 - 20

Centralized + Decentralized
Manageable
Coherent

X Same as decentralized
½ Better than decentralized
fewer hosts that are holding authoritative data

Extensible

✓ Anyone can still join!

Fault Tolerant

✓ Plenty of redundancy

Secure
Lawsuit-proof

Scalable

X Same as decentralized
✓ Still no one to sue

?

Looking very hopefully

Best architecture for P2P networks
Unit 2 - 21

2.3 P2P Applications

Unit 2 - 22

P2P Computing Applications
File sharing
Improves data availability
Replication to compensate for failures.
E.g., Napster, Gnutella, Freenet, KaZaA (FastTrack).
Process sharing
For large-scale computations
Data analysis, data mining, scientific computing
Collaborative environments
For remote real-time human collaboration.
Instant messaging, shared whiteboards, teleconferencing
E.g., Skype, Messenger.

Unit 2 - 23

P2P Technical Challenges
Peer identification
Routing protocols

Network topologies
Peer discovery

Communication/coordination protocols
Quality of service

Security

Unit 2 - 24

Famous Napster Model
Created in 1999 by Shawn Fanning, an 18-year-old student
Napster is P2P application network, which gives its members ability to connect
directly to other members’ computers and search their hard drives for digital music
files to share and trade.
Members download a software package from Napster and install it on their
computers.
The Napster central computer maintains directories of music files of members who
are currently connected to the network. These directories are automatically updated
when a member logs on or off the network.
Whenever a member submits a request to search for a file, the central computer
provides information to the requesting member.
The requesting member can then establish a connection directly with another
member’s computer containing that particular file.
The download of the target file takes place directly between the members’
computers, bypassing the central computer.

Unit 2 - 25

Napster Model

Unit 2 - 26

Napster Model
Over 36 million people joined the Napster community
It rapidly accelerated the development and implementation of other P2P models.
The limitation is that it can only share music files
In July 2001, The Recording Industry Association of America (RIAA), ordered to
shutdown Napster due to the free copying of copyrighted material.

Unit 2 - 27

Other P2P Systems
Napster was ordered to shut down because it maintained a central directory
for its members.

New file-sharing P2P systems bypass the legal problems as they do not
hold a central directory

They do not even need a central server or any company to run the system.
Thus, it is impossible to kill the network.
These new P2P systems include Gnutella, KaZaA, LimeWire , Direct
Connect, etc.

Unit 2 - 28

The network structure of Gnutella
The idea of Gnutella is similar to the ‘search
strategies’ employed by humans.
If you want to get a particular file, you can ask one of
your friends. If he/she does not have the file, he/she
can ask his/her friends. This request will be conveyed
from one person to another until it reaches someone
who has the file.
This piece of information will be routed to you
according to the original path.

Computers in the network have different connection
speeds.
A high-speed computer will connect to many
computers, while the low-speed computer will
connect to only a few computers.

Over the course of time, the network will have a
high-speed computer in the core.

Unit 2 - 29

BitTorrent (Motivation)
An analysis by Xerox Research Center indicated that
50% of all files for sharing were stored on only 1% of the peers.
About 70% of all Gnutella users do not share any files with others.

In other word, all they do is ‘download’. They are referred to as “free loaders”
or “free riders”
When a peer shares a popular file with others
in a P2P network, it will attract a large volume
of traffic. And hence this peer needs to pay
more bandwidth costs for more clients
A large proportion of freeloaders,
will defeat the objective to share
workload in a P2P network.

Unit 2 - 30

BitTorrent (Model)
BitTorrent is a P2P protocol, designed by Bram Cohen, for sharing a large file among
a set of peers.
The term file-sharing means that instead of download the whole file from one peer,
the file can be downloaded from a group of peers,
because the file is divided into number of parts that are distributed among this
group of peers.
File sharing is done in a collaborating process called a torrent.
Each peer participating in a torrent downloads chunks
of the large file from another peer that has it and
uploads chunks of that file to other peers that do
not have it.

The performance will be improved because there is
no way to turn off the upload function of a
BitTorrent program when a computer is downloading.

Unit 2 - 31

BitTorrent (Model)
The set of all peers that takes part in a torrent is referred to as a swarm.
A peer in a swarm that has the complete content file is called a seed.
A peer that has only part of the file and wants to download the rest is called a leech.
In other words, a swarm is a combination
of seeds and leeches.
BitTorrent has gone through
several versions and implementations.
The original one, uses a central
node called a tracker, to track
the operation of the swarm.
The new versions eliminate the
tracker by using DHT (more later).
Unit 2 - 32

BitTorrent – How does it work?
Now assume a new peer wants to download a content of a common file.
The new peer search the Internet for the corresponding metafile of this content. This file
is called a Torrent file.
The Torrent file contains:
File name
# of chunks (pieces), size
checksum
IP address of the Tracker,…etc.

Using this Torrent file, the new peer uses a BitTorrent client application to access the
tracker and receives the addresses of some peers in the torrent, normally called
neighbours.
The new peer is now part of the torrent and can download and upload pieces of the
content file.
Nothing can prevent a peer from leaving the torrent before it has all the pieces and joining
later or not joining again.
The BitTorrent protocol applies a set of policies to provide fairness and to prevent
overloading a peer with requests from other peers.

Unit 2 - 33

BitTorrent – Policies (1)
To avoid overloading and to achieve fairness,
each peer needs to limit its concurrent connection to a number of neighbours; the typical value is
four.
A peer flags a neighbour as unchoked or choked.
It also flags them as interested or uninterested.

This means, the provided neighbors list will be divided into unchoked/choked, and
interested/uninterested.
The unchoked group is the list of peers that the current peer has concurrently connected to; it
continuously uploads and downloads pieces from this group.

The choked group is the list of neighbours that the peer is not currently connected to but may
connect to in the future.

Every 10 seconds, the current peer tries one peer from the interested but choked group for
a better data rate. If it has a better rate than any of the unchoked peers, their status may
be swapped.
This strategy divides the neighbours into subgroups in which those neighbours with compatible
data transfer rates will communicate with each other

Unit 2 - 34

BitTorrent – Policies (2)
To allow a newly joined peer, which does not yet have a piece to share, to also
receive pieces from other peers,
Every 30 seconds the system randomly promotes a single peer, regardless of its
uploading rate, from the choked/uninterested group and flags it as unchoked.
This action is called optimistic unchoking.

A balance between the number of pieces that each peer may have is managed by a
strategy called the rarest-first.

Using this strategy, a peer tries to first download the pieces with the fewest
repeated copies among the neighbours.
In this way, these pieces are circulated faster.

Unit 2 - 35

2.4 P2P and the Internet
(Overlay, Structured, Unstructured)

Unit 2 - 36

P2P Overlays and Network Services
Peers in P2P applications communicate with other peers using messages
transmitted over the Internet or other types of networks.

The protocols of various P2P applications have some common features.
protocols are constructed at the application layer.
peers have a unique identifier, which is the peer ID or peer address.
P2P protocols support some type of message-routing capability.
a message intended for one peer can be transmitted via
intermediate peers to reach the destination peer.

Unit 2 - 37

P2P Overlays and Network Services
To distinguish the operation of the P2P protocol at the application layer
from the behavior of the underlying physical network,
the collection of peer connections in a P2P network is called
a P2P overlay

Next slide shows the correspondence between peers connecting in an
overlay network with the corresponding nodes in the underlying physical
network

Unit 2 - 38

Peers form an overlay network (top) uses network connections in the native network (bottom)
The overlay organization is a logical view that might not directly mirror the physical network

Unit 2 - 39

Overlay Networks Types
Depending on how the nodes in a P2P overlay are linked, the overlay network can be
classified as either
Unstructured or Structured overlay networks
Unstructured Networks

The nodes are linked randomly.
A search in unstructured P2P is not very efficient, and a query may not be resolved.
(more details later)
Gnutella and Freenet are examples of unstructured P2P networks.
Structured Networks
Use a predefined set of rules to link nodes so that a query can be effectively and
efficiently resolved.
The most common technique used for this purpose is the Distributed Hash Table
(DHT).
One popular P2P file sharing protocol that uses the DHT is BitTorrent.

Unit 2 - 40

Distributed Hash Table (DHT)
DHT distributes data items (objects) among a set of nodes according to some
predefined rules.
Each peer in a DHT-based network becomes responsible for a range of data items.
Each data item and the responsible peer is mapped to a point in a large address
space of size 2m. (Most of the DHT implementations use m=160)

The address space is designed using modular arithmetic, which means that the points
in the address space is distributed on a circle with 2m points (0 to 2m – 1) using
clockwise direction as shown:
0

Note:
1. Space range is 0 to 2m -1
2. Calculation is done modulo 2m

(3/4) x 2m

Address space
of size 2m

(1/4) x 2m

(1/2) x 2m
Unit 2 - 41

DHT - Hashing Peer/Object Identifier
The first step in creating the DHT system is to place all peers on the address space
ring.
This is normally done by using a hash function that hashes the peer identifier,
normally its IP address, to an m-bit integer, called a node ID.
node ID = hash (Peer IP address)
DHT uses some of the cryptographic hash functions such as Secure Hash Algorithm
(SHA-1) that are collision resistant.

The name of the object (for example, a file) to be shared is also hashed to an m-bit
integer in the same address space, called a key.
key = hash (Object name)
In the DHT an object is normally related to the pair (key, value) in which the key is
the hash of the object name and the value is the object or a reference to the
object.

Unit 2 - 42

DHT - Storing the Object
There are two strategies for storing the object:
A direct method:
The object is stored in the node whose ID is closest to the key in the ring.
The term closest is defined differently in each protocol.
An indirect method:
The peer that owns the object keeps the object, but a reference to the
object is created and stored in the node whose ID is closest to the key
point.
This means, the physical object and the reference to the object are stored in
two different locations (peers).
Most DHT systems use the indirect method due to efficiency.

In either case, a search mechanism is needed to find the object if the name of the
object is given.

Unit 2 - 43

DHT - Example
The normal value of m is 160, for the purpose of demonstration, we use m = 5.

The node N5 with IP address 110.34.56.23 has a file named “SE3314b-Assignment“ that it wants
to share with its peers.
The file is stored in N5, the key of the file is k14, but
the reference to the file is stored in node N17.
SE3314b-Assignment
Legend

5200

0

key = hash (object name)
node = hash (IP address)
point (potential key or node)

(110.34.56.23)

N2

N29

N5
N25
Key
14

Reference
(110.34.56.23:5200)

N10

N20
N17

(129.100.224.11)

5=hash (110.34.56.23)

ID space of
size 25 (m=5)

K14
14=hash (“SE3314b-Assignment”)

Unit 2 - 44

2.5 Unstructured overlay topology

Unit 2 - 45

Unstructured Overlay
An unstructured P2P network is formed when the overlay links are established
arbitrarily.
Unstructured overlays, for example Gnutella,
organize nodes into a random graph and

use floods or random walks to discover data stored by overlay nodes.
Each node visited during a flood or random walk evaluates the query locally on the
data items that it stores.
Unstructured overlays does not impose any constraints on the node graph or on data
placement,
for example, each node can choose any other node to be its neighbour in the
overlay
Unstructured overlays cannot find rare data items efficiently, and it does not
guarantee that an object can be found if it exists in the overlay.

Unit 2 - 46

Flooding and Expanding Ring
When each peer keeps a list of its neighbors,
and when this neighbor relations are transitive,
we will have connectivity graphs such as the one shown in this figure
In this particular graph:
peers have degree from 2 to 5
Increasing the degree reduces
the diameter of the overlay, but
requires more storage at each peer.
Peers can exchange messages with
other peers in its neighbor list.
Message can be a query that contains
the search criteria, such as a filename or keywords.
We don’t know which peers in the overlay have the information, so to whom the
query will be sent?
Unit 2 - 47

Flooding Algorithm
Simple algorithm: (flooding)
Peer could try sending a query to all its neighbor. If the neighbor peers don’t
have the information, they can in turn forward the request to their neighbors,
and so on.
But, how to prevent messages from circulating endlessly?
Using message identifiers
Attach (TTL) value to
a message to limits its lifetime.

Unit 2 - 48

Flooding Algorithm
FloodForward(Query q, Source p)

// have we seen this query before?
if(q.id  oldIdsQ) return // yes, drop it
oldIdsQ = oldIdsQ  q.id // remember this query
// expiration time reached?

q.TTL = q.TTL – 1
if q.TTL  0 then return // yes, drop it
// no, forward it to remaining neighbors
foreach(s  Neighbors) if(s  p) FloodForward(q,s)

Each peer has a list of neighbors.
It initializes its list of neighbors when it joins the overlay
for example, by getting a copy of the neighbor list of the first peer that it connects to
When the query is satisfied at some peer, a response message is sent to the requesting peer.
If the object is not found quickly, the flooding mechanism continues to propagate the query
message along other paths until the TTL value expires or the query is satisfied.

Unit 2 - 49

Expanding Ring
Flooding mechanism creates substantial redundant messaging, which is inefficient for
the network.
We may start the search with a small TTL value. If this succeeds, the search stops.

Otherwise, the TTL value is increased by a small amount and the query is reissued.
This variation of flooding is called iterative deepening or expanding ring

Unit 2 - 50

Random Walk
To avoid the message overhead of flooding, unstructured overlays can use some type
of random walk.
In random walk a single query message is sent to a randomly selected neighbor.

The message has a TTL value that is decremented at each hop.
If the desired object is found, the search terminates.
Otherwise the query fails, by a timeout or an explicit failure message
The same process may be repeated to another randomly chosen path.
To improve the response time, several random walk queries can be issued in parallel

Unit 2 - 51

Random Walk Algorithm

RandomWalk(source, query, TTL)
if (TTL > 0) {
TTL = TTL – 1
// select next hop at random, don’t send back to source
while((next_hop = neighbors[random()]) == source){}
RandomWalk(source, query, TTL)

}

Unit 2 - 52

Summary
Unstructured overlays have been used in several widely used filesharing systems, despite their inefficiencies.
In the research community there has been much effort to increase
their performance and reduce overhead.

Structured overlays, emerged to address limitations of unstructured
overlays by combining a specific geometrical structure with

appropriate routing and maintenance mechanisms.

Unit 2 - 53

2.6 Structured overlay topology

Unit 2 - 54

Motivation and Categories
The earliest peer-to-peer systems used unstructured overlays that were
easy to implement but had inefficient routing and an inability to locate rare
objects.
These problems turned the attentions to design overlays with routing
mechanisms that
are deterministic and that can provide guarantees on the ability to
locate any object stored in the overlay.

The large majority of these designs used overlays with a specific routing
geometry and are called structured overlays.

Unit 2 - 55

Structured overlays & Directed Searches
Idea:

Assign particular nodes to hold particular content (or pointers to it, like
an information booth)

When a node wants that content, go to the node that is supposed to
have or know about it

Challenges:

Distributed: want to distribute responsibilities among existing nodes in
the overlay
Adaptive: nodes join and leave the P2P overlay

distribute knowledge responsibility to joining nodes
redistribute responsibility knowledge from leaving nodes

Unit 2 - 56

Structured overlays & Directed Searches
Structured overlays support key-based routing such that
object identifiers are mapped to the peer identifier address space and
an object request (lookup message) is routed to the nearest peer in
the peer address space.
P2P systems using key-based routing are called distributed object location
and routing (DOLR) systems.
A specific type of DOLR is a distributed hash table (DHT)

In this Unit, we introduce three of these protocols:
Pastry,
Kademlia, and

Chord.

Unit 2 - 57

DHT - Example
The normal value of m is 160, for the purpose of demonstration, we use m = 5.

The node N5 with IP address 110.34.56.23 has a file named “SE3314b-Assignment“ that it wants
to share with its peers.
The file is stored in N5, the key of the file is k14, but
the reference to the file is stored in node N17.
SE3314b-Assignment
Legend

5200

0

key = hash (object name)
node = hash (IP address)
point (potential key or node)

(110.34.56.23)

N2

N29

N5
N25
Key
14

Reference
(110.34.56.23:5200)

N10

N20
N17

(129.100.224.11)

5=hash (110.34.56.23)

ID space of
size 25 (m=5)

K14
14=hash (“SE3314b-Assignment”)

Unit 2 - 58

Pastry
Pastry is designed by Antony Rowstron and Peter Druschel in 2001, and uses DHT.
Nodes and data items are identified by m-bit IDs that create an identifier space of
size 2m points distributed in a circle in the clockwise direction.
The common value for m is 128. The protocol uses the SHA-1 hashing algorithm
with m = 128.

In Pastry, an identifier is seen as an n-digit string in base 2b in which b is normally 4
and n = (m/b).
For instance, an identifier is a 32-digit number in base 16 (hexadecimal).

A key is stored in the node whose identifier is numerically closest to the key

Unit 2 - 59

Pastry - Routing
Each node in Pastry can resolve a query using two entities: a routing table and a leaf
set.
(2b) columns.
when m = 128 , b = 4, we have 16 columns

Routing Table

3

4

5

6

7

8

9

A

B

C

D

…

…

…

…

…

…

…

…

…

…

…

…

…

E

F

…

2

…

1

0
1

…

n rows.
when m =
128 , b = 4,
we have 32
(128/4) rows

0

…

Common
prefix length

31

For node N, Table [i, j], gives the ID of a node (if it exists) that shares the i leftmost digits with
the ID for N and its (i+1)th digit has a value of j.
The first row, row 0, shows the list of live nodes whose identifiers have no common prefix with N.
The last row, row 31, shows the list of all live nodes that share the leftmost 31 digits with node
N; only the last digit is different.

Unit 2 - 60

Routing Table, Example
Assume the node N ID is (574A234B12E374A2001B23451EEE4BCD)16
then the value of the Table [2, D] can be the identifier of a node such as (57D...).
1

2

3

4

5

6

7

8

9

A

B

C

D

E

F

…

0

…

Common
prefix length

0
1
2

57D…

…

…

…

…

…

…

…

…

…

…

…

…

…

…

…
31

Note that the leftmost two digits are 57, which are common with the first two digits of N,
but the next digit is D, the value corresponding to the Dth column.
If there are more nodes with the prefix 57D, the closest one, according to the proximity
metric, is chosen, and its identifier is inserted in this cell.
The proximity metric is a measurement of closeness determined by the application that
uses the network.
It can be based on the number of hops between the two nodes, the round-trip time
between the two nodes, or other metrics.
Unit 2 - 61

Pastry - Routing
Leaf Set
Another entity used in routing is a set of 2b identifiers (the size of a row in the
routing table) called the leaf set.
The left half of the set is a list of IDs that are numerically smaller than the
current node ID

The right half is a list of IDs that are numerically larger than the current node ID.
The leaf set gives the identifier of 2b-1 live nodes located before the current node
in the ring and the list of 2b-1 nodes located after the current node in the ring.

Unit 2 - 62

Routing table & Leaf Set, Example
K3222
N0002
K3133
K3122

Node ID 2210
2200 2203 2213 2230
Leaf set
0
1
2
3
3122
0 0202 1212
1 2013 2103
2230
2 2203
2213
3
Routing table

N3200

2013 2103 2203 2210
Leaf set
0
1
2
3
3122
0 0202 1103
1 2013 2103
2210
2230
2
2203
3
Routing table

N0101
N0202

N3122

K0203
0
1
2
3

N0301
N0302
N0321

K2233

Node ID 2200

Node ID 0302

m=8

0202 0301 0321 1000
Leaf set
0
1
2
3
1302 2001 3122
0002 0101 0202
0321
0301
Routing table

N1000

b=2

N2230

N1103

N2213
N2210
N2203
N2200
N2103

K1110
N1212

N1220
N2013

K1213

N2001

N1302
K2011

Node ID 2001
1220 1302 2013 2103
Leaf set
0
1
2
3
3200
0 0101 1000
2103 2200
1
2013
2
3
Routing table

Unit 2 - 63

Pastry – Lookup Operation
Pastry - lookup: given a key, it finds the node that stores the information about the
key or the key itself.
Lookup (key)
{
if (key is in the range of N's leaf set)
forward the message to the closest node in the leaf set
else
route (key, Table)
}
route (key, Table)
{
p = length of shared prefix between key and N
v = value of the digit at position p of the key
// Position starts from 0
if (Table [p, v] exists)
forward the message to the node in Table [p, v]
else
forward the message to a node sharing a prefix as long as
the current node, but numerically closer to the key.
}

Unit 2 - 64

Lookup Operation, Example (1)
K3222
N0002
N3200

K3133
K3122

Node ID 2210
2200 2203 2213 2230
Leaf set
0
1
2
3
3122
0 0202 1212
1 2013 2103
2230
2 2203
2213
3
Routing table

(2) Ask 2013
about key
2011

2013 2103 2203 2210
Leaf set
0
1
2
3
3122
0 0202 1103
1 2013 2103
2210
2230
2
2203
3
Routing table

N0101
N0202

N3122

K0203
0
1
2
3

N0301
N0302
N0321

K2233

Node ID 2200

Node ID 0302

m=8

N2230
N2213

N1000

b=2

N1103

(1) Who’s
responsible
for key 2011

N2210
N2203
N2200
N2103

K1110
N1212

N1220
N2013

K1213

N2001

N1302
K2011
(3) Yes, I
have the key
2011

0202 0301 0321 1000
Leaf set
0
1
2
3
1302 2001 3122
0002 0101 0202
0321
0301
Routing table

Node ID 2001
1220 1302 2013 2103
Leaf set
0
1
2
3
3200
0 0101 1000
2103 2200
1
2013
2
3
Routing table

Unit 2 - 65

Lookup Operation, Example (2)

(3) Yes, I
have the key
0203

(1) Who’s
responsible
for key 0203

(2) Ask 0202
about key
0203

Unit 2 - 66

Pastry – Join Operation
The process of joining the ring in Pastry is as follows:
The new node, X, should know at least one node N0, which should be close to X, and
send a join message to it. (we assume that N0 has no common prefix with X)
1. Node N0 sends the contents of its row 0 to node X. Since the two nodes have no
common prefix, node X uses the appropriate parts of this information to build its row 0.
2. Node N0 call a lookup operation with X’s ID as a key, which will forward the join
message to a node, N1, whose identifier is closest to X.
3. Node N1 sends the contents of its row 1 to node X. Since the two nodes have one
common prefix.
4. Node N1 then call a lookup operation with X’s ID as a key, which will forward the join
message to a node, N2, whose identifier is closest to X.
5. The process continues until the routing table of node X is complete.
6. The last node in the process, which has the longest common prefix with X, also sends its
leaf set to node X, which becomes the leaf set of X.

Unit 2 - 67

Join Operation, Example

X

2212

join
N0

0302

Row 0

join

2001

Row 1

join

2200

Row 2

join

2210

Row 3

Node ID 2212
2200 2203 2213 2230
Leaf set
0
1
2
3
1302
3122
0
2103
1
2230
2
2213
3
Routing table

Leaf set

A new node X with node ID N2212 uses the information in four nodes as shown to create its
initial routing table and leaf set for joining the ring.
We assume that node 0302 is a nearby node to node 2212 based on the proximity metric.

Unit 2 - 68

Pastry – Leave (or Fail) Operation
Each Pastry node periodically tests the liveliness of the nodes in its leaf set and
routing table by exchanging probe messages.
If a local node finds that a node in its leaf set is not responding to the probe
message, it assumes that the node has failed or departed.
The local node then contacts the live node in its leaf set with the largest identifier
and repairs its leaf set with the information in the leaf set of that node.
If a local node finds that a node in its routing table, Table [i, j], is not responsive to
the probe message, it sends a message to a live node in the same row and requests
the identifier in Table [i, j] of that node.
This identifier replaces the failed or departed node.

Unit 2 - 69

Kademlia
Kademlia, is a DHT peer-to-peer network that is designed by Petar Maymounkov and
David Mazires, in 2002.
Kademlia routes messages based on the distance between nodes.
The distance between the two identifiers (nodes or keys) is measured as the bitwise
exclusive-or (XOR) between them.

For instance, if x and y are two identifiers, the distance between them is defined as:
distance (x, y) = x  y
This distance function has the following properties:

xx=0

The distance between a node and itself is zero.

x  y > 0 if x ≠ y

The distance between any two distinct nodes is greater than zero.

xy=yx

The distance between x and y is the same as between y and x.

x  z ≤ x  y + y  z Triangular relationship is satisfied.

Unit 2 - 70

Kademlia – Identifier space
Nodes and data items are m-bit identifiers that create an identifier space of 2m points
distributed on the leaves of a binary tree.
The protocol uses the SHA-1 hashing algorithm with m = 160.
For example: if m = 4, we have 16 IDs distributed on the leaves of a binary tree as:
1111

0000

N0

N3 K3 K4

N1

N5

N6

K7 N8

K9

N11

K12

N15

Tree root
1

0
0
0
0

1

N0

N1

1

1
0

0

0
1

N3 K3

1

0

0

1

0

1

0

1

K4

N5

N6

K7

N8

K9

1

1
0

0
1

0

N11

K12

1

1
0

1

N15

k3 is stored in N3 because 3  3 = 0. k7 is stored in N6 not in N8 because 6  7 = 1 but
7  8 = 15. k12 is stored in N15 not in N11 because 11  12 = 7, but 12  15 = 3.

Unit 2 - 71

Kademlia – Routing table
Kademlia keeps only one routing table for each node; there is no leaf set.
Each node N divides the binary tree into m subtrees.
A subtree i includes nodes that share i leftmost bits (common prefix P) with the a
node N, and does not include the node N itself.

For example, the node N5 divides our previous tree as follows:

P1

P3

P2

P0

0101

Unit 2 - 72

Kademlia – Routing table
Routing Table
The routing table is made of m rows but only one column, as follows.
Common
prefix length

Identifiers

0

Closest node(s) in subtree with common prefix of length 0

1

Closest node(s) in subtree with common prefix of length 1

2

Closest node(s) in subtree with common prefix of length 2

…

…
m -1

Closest node(s) in subtree with common prefix of length m - 1

The idea is the same as that used by Pastry, but the length of the common prefix is based
on the number of bits instead of the number of digits in base 2b

Unit 2 - 73

Routing Table, Example
Let us find the routing table for our previous Example. To make the example simple, we assume
that each row uses only one identifier.

P3

P2
To update
row 2

0000

N8
N5
N3
N1

N0

To update
row 0

To update
row 1

To update
row 3

Node N0
0
1
2
3

P0

P1

N8 is the
closest node
to N0

N5 is the
closest node
to N0

N1

N3 K3

K4

N5

N6

K7

N8

K9

N11

K12

N15

Unit 2 - 74

Routing Table, Example
Let us find the routing table for our previous Example. To make the example simple, we assume
that each row uses only one identifier.

P3

P2

P0

P1

0001
Node N0
0
1
2
3

N8
N5
N3
N1

N0

Node N1
0
1
2
3

N1

N8
N5
N3
N0

N3 K3

K4

N5

N6

K7

N8

K9

N11

K12

N15

Unit 2 - 75

Routing Table, Example
Let us find the routing table for our previous Example. To make the example simple, we assume
that each row uses only one identifier.

P2

P3

P0

P1
0011

Node N0
0
1
2
3

N8
N5
N3
N1

N0

Node N1
0
1
2
3

N1

N8
N5
N3
N0

Node N3
0 N11
1 N6
2 N1
3

N3 K3

K4

N5

N6

K7

N8

K9

N11

K12

N15

Unit 2 - 76

Routing Table, Example
Let us find the routing table for our previous Example. To make the example simple, we assume
that each row uses only one identifier.

P3

P1

P2

P0

0101
Node N0
0
1
2
3

N8
N5
N3
N1

N0

Node N1
0
1
2
3

N1

N8
N5
N3
N0

Node N3

Node N5

0 N11
1 N6
2 N1
3

0 N15
1 N1
2 N6
3

N3 K3

K4

N5

N6

K7

N8

K9

N11

K12

N15

Unit 2 - 77

Routing Table, Example
Let us find the routing table for our previous Example. To make the example simple, we assume
that each row uses only one identifier.

P2

P1

P3

P0

0111
Node N0
0
1
2
3

N8
N5
N3
N1

N0

Node N1
0
1
2
3

N1

N8
N5
N3
N0

Node N3

Node N5

Node N6

0 N11
1 N6
2 N1
3

0 N15
1 N1
2 N6
3

0 N15
1 N3
2 N5
3

N3 K3

K4

N5

N6

K7

N8

K9

N11

K12

N15

Unit 2 - 78

Routing Table, Example
Let us find the routing table for our previous Example. To make the example simple, we assume
that each row uses only one identifier.

P0

P3

P2

P1

1000
Node N0
0
1
2
3

N8
N5
N3
N1

N0

Node N1
0
1
2
3

N1

N8
N5
N3
N0

Node N3

Node N5

Node N6

Node N8

0 N11
1 N6
2 N1
3

0 N15
1 N1
2 N6
3

0 N15
1 N3
2 N5
3

0 N0
1 N15
2 N11
3

N3 K3

K4

N5

N6

K7

N8

K9

N11

K12

N15

Unit 2 - 79

Routing Table, Example
Let us find the routing table for our previous Example. To make the example simple, we assume
that each row uses only one identifier.

P0

P2

P3

P1
1011

Node N0
0
1
2
3

N8
N5
N3
N1

N0

Node N1
0
1
2
3

N1

N8
N5
N3
N0

Node N3

Node N5

Node N6

Node N8

Node N11

0 N11
1 N6
2 N1
3

0 N15
1 N1
2 N6
3

0 N15
1 N3
2 N5
3

0 N0
1 N15
2 N11
3

0 N3
1 N15
2 N8
3

N3 K3

K4

N5

N6

K7

N8

K9

N11

K12

N15

Unit 2 - 80

Routing Table, Example
Let us find the routing table for our previous Example. To make the example simple, we assume
that each row uses only one identifier.

P0

P2

P1

P3
1111

Node N0
0
1
2
3

N8
N5
N3
N1

N0

Node N1
0
1
2
3

N1

N8
N5
N3
N0

Node N3

Node N5

Node N6

Node N8

Node N11

Node N15

0 N11
1 N6
2 N1
3

0 N15
1 N1
2 N6
3

0 N15
1 N3
2 N5
3

0 N0
1 N15
2 N11
3

0 N3
1 N15
2 N8
3

0 N6
1 N11
2
3

N3 K3

K4

N5

N6

K7

N8

K9

N11

K12

N15

Unit 2 - 81

Lookup Operation, Example (1)
We assume node N0 (0000)2 receives a lookup message to find the node responsible for k12
(1100)2.
Node N0
0
1
2
3

N8
N5
N3
N1

N0
0000
(1) Who’s
responsible for
key K12 (1100)

Node N1
0
1
2
3

N8
N5
N3
N0

Node N3

Node N5

Node N6

Node N8

Node N11

Node N15

0 N11
1 N6
2 N1
3

0 N15
1 N1
2 N6
3

0 N15
1 N3
2 N5
3

0 N0
1 N15
2 N11
3

0 N3
1 N15
2 N8
3

0 N6
1 N11
2
3

N3 K3
0011

N1
0001
(2) Ask
N8 about
K12

K4

N5
0101

N6
0110

K7

N8
1000

K9

(3) Ask
N15 about
K12

N11
1011

K12

N15
1111
(4) Yes, I
have the key
K12 (1100)

The length of the common prefix between the N0 and K12 is 0. N0 sends the message to the
node in row 0 of its routing table, node N8.

In N8, the length of the common prefix is 1. It checks row1 and send the query to N15, which is
responsible for k12.
The routing process is terminated. The route is N0 → N8 → N15.

Unit 2 - 82

Lookup Operation, Example (2)
We assume node N5 (0101)2 receives a lookup message to find the node responsible for k7
(0111)2.
Node N0
0
1
2
3

N8
N5
N3
N1

N0
0000

Node N1
0
1
2
3

N8
N5
N3
N0

Node N3

Node N5

Node N6

Node N8

Node N11

Node N15

0 N11
1 N6
2 N1
3

0 N15
1 N1
2 N6
3

0 N15
1 N3
2 N5
3

0 N0
1 N15
2 N11
3

0 N3
1 N15
2 N8
3

0 N6
1 N11
2
3

N3 K3
0011

N1
0001
(2) Ask
N6 about
K7

K4

N5
0101

(1) Who’s
responsible for
key K7 (0111)

N6
0110

K7

N8
1000

K9

N11
1011

K12

N15
1111

(3) Yes, I
have the key
K7 (0111)

The length of the common prefix between the N5 and K7 is 2. N5 sends the message to the node
in row 2 of its routing table, node N6, which is responsible for k7.
The routing process is terminated. The route is N5 → N6.

Unit 2 - 83

Lookup Operation, Example (3)
We assume node N11 (1011)2 receives a lookup message to find the node responsible for k4
(0100)2.
Node N0
0
1
2
3

N8
N5
N3
N1

N0
0000

Node N1
0
1
2
3

N8
N5
N3
N0

N1
0001

Node N3

Node N5

Node N6

Node N8

Node N11

Node N15

0 N11
1 N6
2 N1
3

0 N15
1 N1
2 N6
3

0 N15
1 N3
2 N5
3

0 N0
1 N15
2 N11
3

0 N3
1 N15
2 N8
3

0 N6
1 N11
2
3

(3) Ask
N6 about
K4

N3 K3
0011

K4

N5
0101

(5) Yes, I
have the key
K4 (0100)

N6
0110

K7
(4) Ask
N5 about
K4

N8
1000

K9
(2) Ask
N3 about
K4

N11
1011

K12

N15
1111

(1) Who’s
responsible for
key K4 (0100)

The length of the common prefix between the N11 and K4 is 0.
N11 sends the message to the node in row 0 of its routing table, node N3.
In N3, The length of the common prefix between the two identifiers is 1, the message sent to N6.

In N6, The length of the common prefix between the two identifiers is 2, the message sent to N5,
which is responsible for k4.
The routing process is terminated. The route is N11 → N3 → N6 → N5.

Unit 2 - 84

Kademlia – K-Buckets
For more efficiency, Kademlia requires that each row in the routing table keeps at
least up to K=20 nodes from the corresponding subtree.
For this reason, each row in the routing table is referred to as a k-bucket.
Having more than one node in each row allows the node to use an alternative node
when a node leaves the network or fails.

Kademlia keeps those nodes in a bucket that has been connected in the network for
a long time.

Unit 2 - 85

Kademlia – More operations
Join operation
As in Pastry, a node that needs to join the network needs to know at least one
other node.
The joining node sends its identifier to the node as though it is a key to be
found. The response it receives allows the new node to create its k-buckets.
Leave operation
When a node leaves the network or fails, other nodes update their k-buckets
using the lookup process.

Unit 2 - 86

Chord
Chord was published by Stoica et al. in 2001.
Chord uses m-bit number to identify the data items denoted as k (for key) and to
identify the peers denoted as N (for node). .
The identifier space then of size 2m points distributed in a circle in the clockwise
direction.

All arithmetic in the identifier space is done modulo 2m.
Chord recommends the cryptographic hash function SHA-1 for the identifier space
generation. SHA-1 produces output of fixed length equal to 160 bits.

The closest peer with N ≥ k is called the successor of k and hosts the value (k, v).
where, k is the key (hash of the data name) and v is the value (information
about the peer that has the actual object).

Unit 2 - 87

Chord – Finger table
Any node should be able to resolve a query/lookup that asks for the node identifier
responsible for a given key.
If a node has no information about this key it forward the query to another node
that may know.
To do forwarding, each node needs to know about m successors nodes and one
predecessor node.
These information are saved in a routing table called Finger table.
i
A Finger table
of the node N

Target key

Successor of target key

Information about successor

1

N+1

Successor of N+1

IP address and port of successor

2

N+2

Successor of N+2

IP address and port of successor

3

N+4

Successor of N+4

IP address and port of successor

N + 2i-1

Successor of N + 2i-1

IP address and port of successor

…
m

Unit 2 - 88

Finger table - example
Consider a ring with few nodes and m=5, to make the example simpler.
We show only the successor column
from the Finger table for and keys.

Pre: N25
1
2
3
4
5

0
K31

Pre: N20
1 N5
2 N5
3 N5
4 N5
5 N10

K4
N25

Finger

Pre: Predecessor
Finger[1]: Successor

N20
Pre: N12
1 N25
2 N25
3 N25
4 N5
5 N5

Finger

Finger

N5

K26

N10
N10
N10
N20
N25

Pre: N5

K7

1
2
3
4
5

K9
N10

N12
K16

N12
N12
N20
N20
N5

Finger

K14

Note that:
• N5 is responsible for
K26, K31, K4.
• N10 is responsible
for K7, K9.
• N20 is responsible
for K14, K16.

Pre: N10
1
2
3
4
5

N20
N20
N20
N20
N5

Finger

Legend
key = hash (object name)
node = hash (IP address)
point (potential key or node)

Unit 2 - 89

Chord – Lookup Operation
In Chord, the lookup operation is used to find where an object is located among the
available peers in the ring.
To find the object, a peer need to know the node that is responsible for that object
(the peer that stores reference to that object).
We know that, a peer that is the successor of a set of keys in the ring is the
responsible peer for those keys.
Finding the responsible node is actually finding the successor of a key.
To find the successor of a key, the lookup operation
first find the predecessor of the key (using find_predecessor function), and

then from the predecessor node it finds the next node in the ring which is the
value of “finger [1]”.
If the key is located far from the current node, the node needs the help of other
nodes to find the predecessor (using find_closest_predecessor function).

Next slide shows the code for the lookup operation.

Unit 2 - 90

Chord – Lookup Operations
Who’s
responsible
for key K14

Lookup (key)
{
if (the current node N is responsible for the key)
return (N’s ID)
else
return find_successor (key)
}
find_successor (id)
{
x= find_ predecessor (id)
return x.finger[1]
}
find_predecessor (id) {
x= N
while (id  (x, x.finger[1]) {
x = x.find_closest_predecessor (id)
}
return x
}
find_closest_predecessor (id) {
for (i = m downto 1) {
if (finger [i]  (N, id))
return (finger [i])
}
return N
}

// N is the current node
// Let x find it

//N is the current node
//The node itself is closest predecessor

Unit 2 - 91

Chord – Stabilize Operation
Leaving and Joining of a node or a group of nodes may destabilize the ring.
Chord defines an operation called stabilize to address this issue so that:
Each node in the ring periodically uses stabilize
to validate its information about its successor and
let the successor validate its information about its predecessor.

In other words,
1. Node N uses the value of finger[1], S, to ask node S to return its predecessor, P.
2. If the return value, P, from this query is between N and S, this means that there
is a node with ID equals P that lies between N and S.
3. Then node N makes P its successor and notifies P to make node N its
predecessor

Unit 2 - 92

Chord – Stabilize Operation
Stabilize ( )
{
P= finger[1].Pre

//Ask the successor to return its predecessor

if(P  (N, finger[1]))
finger[1] =P
finger[1].notify (N)

// P is the possible successor of N
// Notify P to change its predecessor

}
Notify (x)
{
if (Pre = null or x  (Pre, N))
Pre = x
}

Unit 2 - 93

Chord – Fix_Finger Operation
Destabilization may change the finger table of up to m nodes.
Chord defines a fix_finger function to update its finger tables.
Each node in the ring must periodically call this function.
To avoid traffic on the system, each node must only update one of its fingers in each
call. This finger is chosen randomly.

Fix_Finger ()
{
Generate (i  (1, m])
finger[i] =find_successor (N + 2

// Randomly generate i such that 1< i ≤ m
i– 1)

// Find value of finger[i]

}

Unit 2 - 94

Chord – Join Operation
When a new node (N) joins the ring, it uses the join operation.
Join function needs to know an ID of another node (say x) to find the successor of
the new node and set its predecessor to null.
It immediately calls the stabilize function to validate its successor.
The new node then asks the successor to call the move-key function that transfers
the keys that the new node is responsible for.

Join (x)
{
Initialize (x)
finger[1].Move_Keys (N)
}

Initialize (x)
{
Pre = null
if (x = null) finger[1] = N
else finger[1] = x. Find_Successor (N)
}

Move_Keys (x)
{
for (each key k)
if (x  [k, N)) move (k to node x)
}

// N is the current node

Note that, after this operation, the finger table of
the new (joined) node is empty and the finger table
of up to m predecessors is out of date.
The stabilize and the fix-finger operations that run
periodically after this event will gradually stabilize
the system.

Unit 2 - 95

Join Operation - Example
0
Pre: N20
1 N5
2 N5
3 N5
4 N5
5 N10

Finger

N17

N17

Pre: N12
1 N25
2 N25
3 N25
4 N5
5 N5

Finger
1. N17 needs to join with the help of N5.
2. N17 uses Initialize (5) – to set its predecessor to null and its
successor (finger[1]) to N20.
3. N17 then asks N20 to send k14 and k16 to N17 because N17 is now
responsible for these keys
4. N17 uses stabilize to asks N20 to change its predecessor to N17
5. When N12 uses stabilize, the predecessor of N17 is updated to N12
6. Finally, when some nodes use fix-finger, the finger table of nodes
N17, N10, N5, and N12 is changed

Pre: N25

K31

K4
N5

K26
N25

Pre: Predecessor
Finger[1]: Successor

K22
N20
N12
Pre: null
N20
N25
N25
N05

N10
N10
N10
N20
N25

N17
Pre: N5

Finger

N17

K9
N10

N12
N17 K16 K14

1 N20
2
3
4
5

K7

1
2
3
4
5

1
2
3
4
5

N12
N12
N20
N20
N5

Finger
Pre: N10
1
2
3
4
5

N20
N20
N20
N20
N5

N17
N17
N17

Finger

Finger

Unit 2 - 96

Chord – Leave (or Fail) Operation
When a peer leaves the ring or the peer fails, the status of the ring will be disrupted
unless the ring stabilizes itself.
Each node exchanges ping and pong messages with neighbours to find out if they are
alive.
When a node does not receive a pong message in response to its ping message, the
node knows that the neighbour is dead.
The node that detects the problem can immediately launch these stabilize and fix
finger operations.
Note that, the data managed by the node that left or failed is no longer available.

Therefore, Chord requires that data and references be duplicated on other
nodes.

Unit 2 - 97

Leave Operation - Example
N12
N12
N12

To leave

N12

N5

1. Node N5 finds out about N10's departure when it does not receive a
pong message to its ping message.
2. Node N5 changes its successor (finger[1]) to N12.
3. Node N5 immediately launches the stabilize function and asks N12 to
change its predecessor to N5.
4. Hopefully, k7 and k9, which were under the responsibility of N10,
have been duplicated in N12 before the departure of N10.
5. After a few calls of fix-finger, nodes N5 and N25 update their finger
tables as shown in the figure.

Unit 2 - 98

Whiteboard

